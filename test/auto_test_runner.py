"""å…¨è‡ªåŠ¨æµ‹è¯•è¿è¡Œå™¨

æä¾›è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œã€é”™è¯¯åˆ†æå’Œä¿®å¤å»ºè®®åŠŸèƒ½ã€‚
æ”¯æŒ AI è¾…åŠ©çš„é”™è¯¯è¯Šæ–­å’Œè‡ªåŠ¨ä¿®å¤ã€‚
"""
import subprocess
import sys
import json
import re
import time
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple
from enum import Enum


class TestStatus(Enum):
    """æµ‹è¯•çŠ¶æ€æšä¸¾"""
    PASSED = "passed"
    FAILED = "failed"
    ERROR = "error"
    SKIPPED = "skipped"


@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    name: str
    status: TestStatus
    duration: float = 0.0
    error_message: str = ""
    error_traceback: str = ""
    file_path: str = ""
    line_number: int = 0


@dataclass
class ErrorAnalysis:
    """é”™è¯¯åˆ†æç»“æœ"""
    error_type: str
    error_message: str
    file_path: str
    line_number: int
    suggested_fix: str
    fix_code: str = ""
    confidence: float = 0.0


@dataclass
class TestReport:
    """æµ‹è¯•æŠ¥å‘Š"""
    total: int = 0
    passed: int = 0
    failed: int = 0
    errors: int = 0
    skipped: int = 0
    duration: float = 0.0
    results: List[TestResult] = field(default_factory=list)
    error_analyses: List[ErrorAnalysis] = field(default_factory=list)
    timestamp: str = ""


class AutoTestRunner:
    """å…¨è‡ªåŠ¨æµ‹è¯•è¿è¡Œå™¨
    
    åŠŸèƒ½ï¼š
    1. è‡ªåŠ¨å‘ç°å’Œè¿è¡Œæµ‹è¯•
    2. åˆ†ææµ‹è¯•å¤±è´¥åŸå› 
    3. æä¾›ä¿®å¤å»ºè®®
    4. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    """
    
    def __init__(self, project_root: Optional[Path] = None):
        self.project_root = project_root or Path(__file__).parent.parent
        self.test_dir = self.project_root / "test"
        self.report: Optional[TestReport] = None
    
    def run_all_tests(self, verbose: bool = True) -> TestReport:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("=" * 60)
        print("ğŸš€ å¼€å§‹è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•...")
        print("=" * 60)
        
        start_time = time.time()
        
        # è¿è¡Œ pytest å¹¶æ”¶é›†ç»“æœ
        result = self._run_pytest(verbose)
        
        # è§£ææµ‹è¯•ç»“æœ
        self.report = self._parse_results(result)
        self.report.duration = time.time() - start_time
        self.report.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        
        # åˆ†æé”™è¯¯
        if self.report.failed > 0 or self.report.errors > 0:
            self._analyze_errors()
        
        # æ‰“å°æŠ¥å‘Š
        self._print_report()
        
        return self.report
    
    def run_specific_tests(self, test_pattern: str) -> TestReport:
        """è¿è¡Œç‰¹å®šæµ‹è¯•"""
        print(f"ğŸ” è¿è¡ŒåŒ¹é… '{test_pattern}' çš„æµ‹è¯•...")
        
        start_time = time.time()
        result = self._run_pytest(True, extra_args=["-k", test_pattern])
        
        self.report = self._parse_results(result)
        self.report.duration = time.time() - start_time
        self.report.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        
        if self.report.failed > 0 or self.report.errors > 0:
            self._analyze_errors()
        
        self._print_report()
        return self.report
    
    def _run_pytest(
        self, 
        verbose: bool = True, 
        extra_args: Optional[List[str]] = None
    ) -> subprocess.CompletedProcess:
        """è¿è¡Œ pytest"""
        cmd = [
            sys.executable, "-m", "pytest",
            str(self.test_dir),
            "--tb=short",
            "-q" if not verbose else "-v",
            "--no-header",
        ]
        
        if extra_args:
            cmd.extend(extra_args)
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=str(self.project_root),
                timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
            )
            return result
        except subprocess.TimeoutExpired:
            print("âš ï¸ æµ‹è¯•æ‰§è¡Œè¶…æ—¶")
            return subprocess.CompletedProcess(cmd, 1, "", "Timeout")
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
            return subprocess.CompletedProcess(cmd, 1, "", str(e))
    
    def _parse_results(self, result: subprocess.CompletedProcess) -> TestReport:
        """è§£æ pytest è¾“å‡º"""
        report = TestReport()
        output = result.stdout + result.stderr
        
        # è§£ææµ‹è¯•ç»“æœè¡Œ
        test_pattern = re.compile(
            r'(test_\w+\.py::\w+(?:::\w+)*)\s+(PASSED|FAILED|ERROR|SKIPPED)'
        )
        
        for match in test_pattern.finditer(output):
            test_name = match.group(1)
            status_str = match.group(2)
            status = TestStatus[status_str]
            
            test_result = TestResult(
                name=test_name,
                status=status,
                file_path=test_name.split("::")[0]
            )
            report.results.append(test_result)
            
            if status == TestStatus.PASSED:
                report.passed += 1
            elif status == TestStatus.FAILED:
                report.failed += 1
            elif status == TestStatus.ERROR:
                report.errors += 1
            elif status == TestStatus.SKIPPED:
                report.skipped += 1
        
        report.total = len(report.results)
        
        # å¦‚æœæ²¡æœ‰è§£æåˆ°ç»“æœï¼Œå°è¯•ä»æ‘˜è¦è¡Œè§£æ
        if report.total == 0:
            summary_pattern = re.compile(
                r'(\d+)\s+passed|(\d+)\s+failed|(\d+)\s+error|(\d+)\s+skipped'
            )
            for match in summary_pattern.finditer(output):
                if match.group(1):
                    report.passed = int(match.group(1))
                if match.group(2):
                    report.failed = int(match.group(2))
                if match.group(3):
                    report.errors = int(match.group(3))
                if match.group(4):
                    report.skipped = int(match.group(4))
            report.total = report.passed + report.failed + report.errors + report.skipped
        
        # æå–é”™è¯¯ä¿¡æ¯
        self._extract_error_details(output, report)
        
        return report
    
    def _extract_error_details(self, output: str, report: TestReport) -> None:
        """æå–é”™è¯¯è¯¦æƒ…"""
        # åŒ¹é…å¤±è´¥æµ‹è¯•çš„é”™è¯¯ä¿¡æ¯
        failure_pattern = re.compile(
            r'FAILED\s+(test_\w+\.py::\w+(?:::\w+)*)\s*-\s*(.+?)(?=\n(?:FAILED|PASSED|ERROR|=|$))',
            re.DOTALL
        )
        
        for match in failure_pattern.finditer(output):
            test_name = match.group(1)
            error_info = match.group(2).strip()
            
            # æ›´æ–°å¯¹åº”æµ‹è¯•ç»“æœçš„é”™è¯¯ä¿¡æ¯
            for result in report.results:
                if result.name == test_name:
                    result.error_message = error_info
                    break
    
    def _analyze_errors(self) -> None:
        """åˆ†ææµ‹è¯•é”™è¯¯å¹¶ç”Ÿæˆä¿®å¤å»ºè®®"""
        if not self.report:
            return
        
        for result in self.report.results:
            if result.status in [TestStatus.FAILED, TestStatus.ERROR]:
                analysis = self._analyze_single_error(result)
                if analysis:
                    self.report.error_analyses.append(analysis)
    
    def _analyze_single_error(self, result: TestResult) -> Optional[ErrorAnalysis]:
        """åˆ†æå•ä¸ªé”™è¯¯"""
        error_msg = result.error_message.lower()
        
        # å¸¸è§é”™è¯¯æ¨¡å¼å’Œä¿®å¤å»ºè®®
        error_patterns = [
            {
                "pattern": "importerror",
                "type": "ImportError",
                "suggestion": "æ£€æŸ¥æ¨¡å—å¯¼å…¥è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿ä¾èµ–å·²å®‰è£…",
                "fix_hint": "pip install <missing_module> æˆ–æ£€æŸ¥ç›¸å¯¹å¯¼å…¥è·¯å¾„"
            },
            {
                "pattern": "modulenotfounderror",
                "type": "ModuleNotFoundError", 
                "suggestion": "æ¨¡å—æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥åŒ…åæ˜¯å¦æ­£ç¡®æˆ–æ˜¯å¦å·²å®‰è£…",
                "fix_hint": "pip install <module_name>"
            },
            {
                "pattern": "attributeerror",
                "type": "AttributeError",
                "suggestion": "å¯¹è±¡æ²¡æœ‰è¯¥å±æ€§ï¼Œæ£€æŸ¥å±æ€§åæ‹¼å†™æˆ–å¯¹è±¡ç±»å‹",
                "fix_hint": "æ£€æŸ¥å¯¹è±¡æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–ï¼Œå±æ€§åæ˜¯å¦æ­£ç¡®"
            },
            {
                "pattern": "typeerror",
                "type": "TypeError",
                "suggestion": "ç±»å‹é”™è¯¯ï¼Œæ£€æŸ¥å‡½æ•°å‚æ•°ç±»å‹æˆ–æ“ä½œæ•°ç±»å‹",
                "fix_hint": "æ£€æŸ¥å‚æ•°ç±»å‹æ˜¯å¦åŒ¹é…å‡½æ•°ç­¾å"
            },
            {
                "pattern": "assertionerror",
                "type": "AssertionError",
                "suggestion": "æ–­è¨€å¤±è´¥ï¼Œæ£€æŸ¥æµ‹è¯•æœŸæœ›å€¼æ˜¯å¦æ­£ç¡®",
                "fix_hint": "æ£€æŸ¥å®é™…å€¼ä¸æœŸæœ›å€¼ï¼Œå¯èƒ½éœ€è¦æ›´æ–°æµ‹è¯•æˆ–ä¿®å¤ä»£ç "
            },
            {
                "pattern": "keyerror",
                "type": "KeyError",
                "suggestion": "å­—å…¸é”®ä¸å­˜åœ¨ï¼Œæ£€æŸ¥é”®åæˆ–ä½¿ç”¨ .get() æ–¹æ³•",
                "fix_hint": "ä½¿ç”¨ dict.get(key, default) æˆ–æ£€æŸ¥é”®æ˜¯å¦å­˜åœ¨"
            },
            {
                "pattern": "valueerror",
                "type": "ValueError",
                "suggestion": "å€¼é”™è¯¯ï¼Œæ£€æŸ¥ä¼ å…¥çš„å€¼æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…",
                "fix_hint": "æ·»åŠ è¾“å…¥éªŒè¯æˆ–æ£€æŸ¥å€¼çš„æœ‰æ•ˆæ€§"
            },
            {
                "pattern": "connectionerror|timeout",
                "type": "NetworkError",
                "suggestion": "ç½‘ç»œè¿æ¥é”™è¯¯ï¼Œæ£€æŸ¥ç½‘ç»œæˆ–ä½¿ç”¨ mock",
                "fix_hint": "åœ¨æµ‹è¯•ä¸­ä½¿ç”¨ mock æ›¿ä»£çœŸå®ç½‘ç»œè¯·æ±‚"
            },
            {
                "pattern": "filenotfounderror",
                "type": "FileNotFoundError",
                "suggestion": "æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œæ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                "fix_hint": "æ£€æŸ¥æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿æµ‹è¯•æ–‡ä»¶å­˜åœ¨"
            },
        ]
        
        for pattern_info in error_patterns:
            if re.search(pattern_info["pattern"], error_msg):
                return ErrorAnalysis(
                    error_type=pattern_info["type"],
                    error_message=result.error_message,
                    file_path=result.file_path,
                    line_number=result.line_number,
                    suggested_fix=pattern_info["suggestion"],
                    fix_code=pattern_info["fix_hint"],
                    confidence=0.8
                )
        
        # é»˜è®¤åˆ†æ
        return ErrorAnalysis(
            error_type="Unknown",
            error_message=result.error_message,
            file_path=result.file_path,
            line_number=result.line_number,
            suggested_fix="æ£€æŸ¥é”™è¯¯ä¿¡æ¯ï¼Œå®šä½é—®é¢˜æ ¹æº",
            confidence=0.3
        )
    
    def _print_report(self) -> None:
        """æ‰“å°æµ‹è¯•æŠ¥å‘Š"""
        if not self.report:
            return
        
        print("\n" + "=" * 60)
        print("ğŸ“Š æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {self.report.duration:.2f}s")
        print(f"ğŸ“… æ—¶é—´æˆ³: {self.report.timestamp}")
        print("-" * 60)
        print(f"ğŸ“ˆ æ€»è®¡: {self.report.total} ä¸ªæµ‹è¯•")
        print(f"   âœ… é€šè¿‡: {self.report.passed}")
        print(f"   âŒ å¤±è´¥: {self.report.failed}")
        print(f"   âš ï¸  é”™è¯¯: {self.report.errors}")
        print(f"   â­ï¸  è·³è¿‡: {self.report.skipped}")
        
        # è®¡ç®—é€šè¿‡ç‡
        if self.report.total > 0:
            pass_rate = (self.report.passed / self.report.total) * 100
            print(f"   ğŸ“Š é€šè¿‡ç‡: {pass_rate:.1f}%")
        
        # æ‰“å°é”™è¯¯åˆ†æ
        if self.report.error_analyses:
            print("\n" + "-" * 60)
            print("ğŸ” é”™è¯¯åˆ†æä¸ä¿®å¤å»ºè®®")
            print("-" * 60)
            
            for i, analysis in enumerate(self.report.error_analyses, 1):
                print(f"\n[{i}] {analysis.error_type}")
                print(f"    ğŸ“ æ–‡ä»¶: {analysis.file_path}")
                print(f"    ğŸ’¬ é”™è¯¯: {analysis.error_message[:100]}...")
                print(f"    ğŸ’¡ å»ºè®®: {analysis.suggested_fix}")
                print(f"    ğŸ”§ ä¿®å¤: {analysis.fix_code}")
                print(f"    ğŸ“Š ç½®ä¿¡åº¦: {analysis.confidence * 100:.0f}%")
        
        print("\n" + "=" * 60)
        
        # æœ€ç»ˆçŠ¶æ€
        if self.report.failed == 0 and self.report.errors == 0:
            print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        else:
            print("âŒ å­˜åœ¨æµ‹è¯•å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šæ–¹é”™è¯¯åˆ†æ")
        
        print("=" * 60)
    
    def generate_json_report(self, output_path: Optional[Path] = None) -> str:
        """ç”Ÿæˆ JSON æ ¼å¼æŠ¥å‘Š"""
        if not self.report:
            return "{}"
        
        report_dict = {
            "summary": {
                "total": self.report.total,
                "passed": self.report.passed,
                "failed": self.report.failed,
                "errors": self.report.errors,
                "skipped": self.report.skipped,
                "duration": self.report.duration,
                "timestamp": self.report.timestamp,
                "pass_rate": (self.report.passed / self.report.total * 100) if self.report.total > 0 else 0
            },
            "results": [
                {
                    "name": r.name,
                    "status": r.status.value,
                    "error_message": r.error_message,
                    "file_path": r.file_path
                }
                for r in self.report.results
            ],
            "error_analyses": [
                {
                    "error_type": a.error_type,
                    "error_message": a.error_message,
                    "file_path": a.file_path,
                    "suggested_fix": a.suggested_fix,
                    "fix_code": a.fix_code,
                    "confidence": a.confidence
                }
                for a in self.report.error_analyses
            ]
        }
        
        json_str = json.dumps(report_dict, ensure_ascii=False, indent=2)
        
        if output_path:
            output_path.write_text(json_str, encoding="utf-8")
            print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
        
        return json_str


class AutoFixer:
    """è‡ªåŠ¨ä¿®å¤å™¨
    
    åŸºäºé”™è¯¯åˆ†æç»“æœï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜ã€‚
    """
    
    def __init__(self, project_root: Optional[Path] = None):
        self.project_root = project_root or Path(__file__).parent.parent
    
    def suggest_fixes(self, analyses: List[ErrorAnalysis]) -> List[Dict]:
        """æ ¹æ®é”™è¯¯åˆ†æç”Ÿæˆä¿®å¤å»ºè®®"""
        fixes = []
        
        for analysis in analyses:
            fix = {
                "error_type": analysis.error_type,
                "file_path": analysis.file_path,
                "suggestion": analysis.suggested_fix,
                "auto_fixable": self._is_auto_fixable(analysis),
                "fix_steps": self._generate_fix_steps(analysis)
            }
            fixes.append(fix)
        
        return fixes
    
    def _is_auto_fixable(self, analysis: ErrorAnalysis) -> bool:
        """åˆ¤æ–­é”™è¯¯æ˜¯å¦å¯ä»¥è‡ªåŠ¨ä¿®å¤"""
        auto_fixable_types = [
            "ImportError",
            "ModuleNotFoundError",
        ]
        return analysis.error_type in auto_fixable_types and analysis.confidence > 0.7
    
    def _generate_fix_steps(self, analysis: ErrorAnalysis) -> List[str]:
        """ç”Ÿæˆä¿®å¤æ­¥éª¤"""
        steps = []
        
        if analysis.error_type == "ImportError":
            steps = [
                "1. æ£€æŸ¥å¯¼å…¥è¯­å¥çš„æ¨¡å—è·¯å¾„",
                "2. ç¡®è®¤æ¨¡å—æ˜¯å¦å·²å®‰è£…: pip list | grep <module>",
                "3. å¦‚æœæ˜¯ç›¸å¯¹å¯¼å…¥ï¼Œæ£€æŸ¥ __init__.py æ–‡ä»¶",
                "4. å°è¯•ä½¿ç”¨ç»å¯¹å¯¼å…¥æˆ–ä¿®æ­£ç›¸å¯¹å¯¼å…¥å±‚çº§"
            ]
        elif analysis.error_type == "ModuleNotFoundError":
            steps = [
                "1. å®‰è£…ç¼ºå¤±çš„æ¨¡å—: pip install <module>",
                "2. æ£€æŸ¥ requirements.txt æ˜¯å¦åŒ…å«è¯¥ä¾èµ–",
                "3. ç¡®è®¤è™šæ‹Ÿç¯å¢ƒæ˜¯å¦æ­£ç¡®æ¿€æ´»"
            ]
        elif analysis.error_type == "AssertionError":
            steps = [
                "1. æ£€æŸ¥æµ‹è¯•çš„æœŸæœ›å€¼æ˜¯å¦æ­£ç¡®",
                "2. è¿è¡Œè¢«æµ‹ä»£ç ï¼Œç¡®è®¤å®é™…è¾“å‡º",
                "3. æ›´æ–°æµ‹è¯•ç”¨ä¾‹æˆ–ä¿®å¤ä»£ç é€»è¾‘"
            ]
        elif analysis.error_type == "AttributeError":
            steps = [
                "1. æ£€æŸ¥å¯¹è±¡ç±»å‹æ˜¯å¦æ­£ç¡®",
                "2. ç¡®è®¤å±æ€§åæ‹¼å†™",
                "3. æ£€æŸ¥å¯¹è±¡æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–"
            ]
        elif analysis.error_type == "TypeError":
            steps = [
                "1. æ£€æŸ¥å‡½æ•°å‚æ•°ç±»å‹",
                "2. ç¡®è®¤å‚æ•°æ•°é‡æ˜¯å¦æ­£ç¡®",
                "3. æ£€æŸ¥æ˜¯å¦é—æ¼äº†å¿…éœ€å‚æ•°"
            ]
        else:
            steps = [
                "1. ä»”ç»†é˜…è¯»é”™è¯¯ä¿¡æ¯",
                "2. å®šä½é”™è¯¯å‘ç”Ÿçš„ä»£ç è¡Œ",
                "3. æ£€æŸ¥ç›¸å…³ä»£ç é€»è¾‘",
                "4. å‚è€ƒæ–‡æ¡£æˆ–æœç´¢ç±»ä¼¼é—®é¢˜"
            ]
        
        return steps


class ContinuousTestRunner:
    """æŒç»­æµ‹è¯•è¿è¡Œå™¨
    
    ç›‘æ§æ–‡ä»¶å˜åŒ–ï¼Œè‡ªåŠ¨è¿è¡Œç›¸å…³æµ‹è¯•ã€‚
    """
    
    def __init__(self, project_root: Optional[Path] = None):
        self.project_root = project_root or Path(__file__).parent.parent
        self.runner = AutoTestRunner(project_root)
    
    def get_related_tests(self, changed_file: str) -> List[str]:
        """è·å–ä¸å˜æ›´æ–‡ä»¶ç›¸å…³çš„æµ‹è¯•"""
        file_path = Path(changed_file)
        file_name = file_path.stem
        
        related = []
        test_dir = self.project_root / "test"
        
        # æŸ¥æ‰¾ç›´æ¥ç›¸å…³çš„æµ‹è¯•æ–‡ä»¶
        for test_file in test_dir.glob("test_*.py"):
            if file_name in test_file.stem:
                related.append(str(test_file))
        
        # å¦‚æœæ˜¯æ¨¡å‹æ–‡ä»¶ï¼Œè¿è¡Œæ‰€æœ‰æ¨¡å‹æµ‹è¯•
        if "model" in file_name.lower():
            related.append(str(test_dir / "test_models.py"))
        
        # å¦‚æœæ˜¯æœåŠ¡æ–‡ä»¶ï¼Œè¿è¡ŒæœåŠ¡æµ‹è¯•
        if "service" in file_name.lower():
            related.append(str(test_dir / "test_services.py"))
        
        # å¦‚æœæ˜¯ API ç›¸å…³ï¼Œè¿è¡Œ API æµ‹è¯•
        if "api" in file_name.lower() or "client" in file_name.lower():
            related.append(str(test_dir / "test_api_client.py"))
        
        return list(set(related))
    
    def run_related_tests(self, changed_file: str) -> TestReport:
        """è¿è¡Œä¸å˜æ›´æ–‡ä»¶ç›¸å…³çš„æµ‹è¯•"""
        related = self.get_related_tests(changed_file)
        
        if not related:
            print(f"âš ï¸ æœªæ‰¾åˆ°ä¸ {changed_file} ç›¸å…³çš„æµ‹è¯•")
            return TestReport()
        
        print(f"ğŸ” è¿è¡Œä¸ {changed_file} ç›¸å…³çš„æµ‹è¯•:")
        for test in related:
            print(f"   - {Path(test).name}")
        
        # æ„å»ºæµ‹è¯•æ¨¡å¼
        pattern = " or ".join(Path(t).stem for t in related)
        return self.runner.run_specific_tests(pattern)


def run_full_test_suite() -> bool:
    """è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶"""
    runner = AutoTestRunner()
    
    print("=" * 60)
    print("ğŸš€ è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    report = runner.run_all_tests(verbose=True)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_path = Path("test_report.json")
    runner.generate_json_report(report_path)
    
    return report.failed == 0 and report.errors == 0


def run_quick_validation() -> bool:
    """å¿«é€ŸéªŒè¯æµ‹è¯•"""
    runner = AutoTestRunner()
    
    print("=" * 60)
    print("âš¡ å¿«é€ŸéªŒè¯æµ‹è¯•")
    print("=" * 60)
    
    # åªè¿è¡Œå•å…ƒæµ‹è¯•
    report = runner.run_specific_tests("not integration and not e2e and not slow")
    
    return report.failed == 0 and report.errors == 0


def main():
    """ä¸»å‡½æ•° - è¿è¡Œè‡ªåŠ¨åŒ–æµ‹è¯•"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="DuanjuApp è‡ªåŠ¨åŒ–æµ‹è¯•è¿è¡Œå™¨",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python auto_test_runner.py              # è¿è¡Œæ‰€æœ‰æµ‹è¯•
  python auto_test_runner.py -v           # è¯¦ç»†æ¨¡å¼
  python auto_test_runner.py -k search    # åªè¿è¡Œæœç´¢ç›¸å…³æµ‹è¯•
  python auto_test_runner.py -q           # å¿«é€ŸéªŒè¯
  python auto_test_runner.py -f src/xxx.py  # è¿è¡Œä¸æ–‡ä»¶ç›¸å…³çš„æµ‹è¯•
  python auto_test_runner.py -o report.json # ç”ŸæˆæŠ¥å‘Š
        """
    )
    parser.add_argument(
        "-k", "--keyword",
        help="åªè¿è¡ŒåŒ¹é…å…³é”®è¯çš„æµ‹è¯•",
        default=None
    )
    parser.add_argument(
        "-v", "--verbose",
        help="è¯¦ç»†è¾“å‡º",
        action="store_true"
    )
    parser.add_argument(
        "-o", "--output",
        help="è¾“å‡º JSON æŠ¥å‘Šçš„è·¯å¾„",
        default=None
    )
    parser.add_argument(
        "-q", "--quick",
        help="å¿«é€ŸéªŒè¯æ¨¡å¼",
        action="store_true"
    )
    parser.add_argument(
        "-f", "--file",
        help="è¿è¡Œä¸æŒ‡å®šæ–‡ä»¶ç›¸å…³çš„æµ‹è¯•",
        default=None
    )
    parser.add_argument(
        "--full",
        help="è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶",
        action="store_true"
    )
    
    args = parser.parse_args()
    
    success = True
    
    if args.quick:
        success = run_quick_validation()
    elif args.full:
        success = run_full_test_suite()
    elif args.file:
        runner = ContinuousTestRunner()
        report = runner.run_related_tests(args.file)
        success = report.failed == 0 and report.errors == 0
    else:
        runner = AutoTestRunner()
        
        if args.keyword:
            report = runner.run_specific_tests(args.keyword)
        else:
            report = runner.run_all_tests(verbose=args.verbose)
        
        if args.output:
            runner.generate_json_report(Path(args.output))
        
        success = report.failed == 0 and report.errors == 0
    
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()

